---
title: 深度学习：深度学习总体概述
date: 2021-05-01 20:39:43
img: /images/4.JPG
author: Yuri
top: false
summary: 深度学习个人的总体概括
categories: 深度学习
tags: 
   - 深度学习
---
## 前言
在学习深度学习和机器学习也有一段时间了终于有点时间来写写笔记回顾一下之前学到的，本文主要是笔者在阅读和观看一系列深度学习相关的书籍或者视频之后写下自己对深度学习的总体内容概述，如有不全或错误之处还请谅解并指出。

## **一、神经网络**

深度学习事实上本质是在研究神经网络，其实深度学习本来就是机器学习的一个分支而机器学习分支中的神经网络由于发展较为迅速并逐渐形成新的学名——深度学习，神经网络和传统的机器学习算法其实个人认为主要的区别是在速度上和准确率上，传统的机器学习算法是在研究各种特征细节，人为能够很好的看到和知道其中的算法原理以及为什么，但深度学习给人的感觉就是一个黑匣子一样，你只知道输入和输出什么，以及中间过程要提供什么样的核等，但对中间计算处理过程很不清楚很模糊。总体而言，个人人为神经网络不仅可以做数据拟合还可以做目标分类识别等，它的能力比以往传统的机器学习还要强大。

## **二、前向神经网络**

深度学习中的第一部分前向神经网络其实可以分为多层感知机，受限玻尔兹曼机，卷积神经网络，深度残差网络等
**1、卷积神经网络**
卷积神经网络的兴起应该是在ImageNet比赛中，应该是2012年的AlexNet卷积神经网络让世界耳目一新，但其实卷积神经网络很早就已经有了雏形，只不过在早些年的硬件技术上和计算能力上根本满足不了网络的实现。典型的CNN模型有AlexNet，LeNet，VGGNet，ResNet等等，这些都是过去几年来很经典并且效果极好的网络模型。
卷积神经网络的发明真的很好的解决了高分辨率图像中参数多，复杂的问题，因为在之前的最基础的神经网络也就是BP神经网络，其输入神经元和中间层都是以全连接的方式进行，这样在以图像为输入数据的神经网络中就会有一个问题，如果图像较大那么平铺展开图像输入进网络那简单的几层网络就会有大量的参数，由此产生了卷积神经网络这种以共享权值作为参数核的网络结构，并且效果好和高效，解决了全连接网络参数多的问题。
**2、多层感知机**
多层感知机就是最早的全连接神经网络，只有简单的几层网络，如图所示
![](https://img-blog.csdnimg.cn/2020112821035464.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70)
![](https://img-blog.csdnimg.cn/20201128210446521.png)

## **三、循环神经网络**

循环神经网络是一类用于处理序列数据结构的网络结构，输入通常是连续长度不固定的序列数据，循环神经网络主要处理序列信息，并且捕获样本之间的关联信息，循环神经网络往往在NLP自然语言处理领域有着较为大的发挥，当然还有语音识别，文字预测等，下图是普通的RNN的一个网络结构图
![](https://img-blog.csdnimg.cn/20201128210852606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70)
**1、LSTM长短时记忆模型**
LSTM最主要的是不仅能够记忆短时信息，而且还能对有用的信息进行长期记忆过滤无用信息，提升了网络的学习能力
![](https://img-blog.csdnimg.cn/20201128211621468.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70)
**2、GRU门控网络**
GRU是对LSTM循环神经网络的一个变型，主要是解决了LSTM的梯度消失和爆炸问题
![](https://img-blog.csdnimg.cn/20201128211713925.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70)

## **四、激活函数**

个人认为激活函数的目的主要是为了使得网络能够具有非线性能力，能够使得网络拟合数据，如果没有激活函数只有单纯的连接，那么网络不管怎么构建都只是一个线性网络，这样的神经网络就失去了它最本质的核心。
**1、Sigmoid激活函数**
Sigmoid函数的表达式
![](https://img-blog.csdnimg.cn/20201128213429173.png)

数学图形式
![](https://img-blog.csdnimg.cn/20201128212254782.png)
可以看到其数学特性，输出在0-1之间，输入为任意参数
但是Sigmoid函数的缺点就是存在梯度消失或梯度爆炸的问题，而且不仅如此，sigmoid函数的输入在-4，+4之间变换较为大，但超出该范围的输出变换基本将趋于0，这样在数据拟合上就显得更加局限，
**2、tanh函数**
tanh函数数学表达式和图形
![](https://img-blog.csdnimg.cn/20201128213417647.png)
![](https://img-blog.csdnimg.cn/20201128213451425.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70)
可以看出来tanh函数其实就是sigmoid的拓展版，它在输出上具有更大的变化性，但是和sigmoid函数一样还是存在着梯度爆炸和消失的问题
**3、ReLu函数**
ReLu函数可以说是深度学习或神经网络中特别常用的一个函数它的构造比较简单
![](https://img-blog.csdnimg.cn/2020112821364085.png)
![](https://img-blog.csdnimg.cn/2020112821370133.png)
可以看出在输入小于0的时候其输出为0，它的梯度较为平稳，不管怎么叠加，不会出现梯度消失或者爆炸问题，但是这也存在另一个问题，它的负领域可能会出现神经元死亡的问题，由此产生了变形的Relu函数比如Leaky Relu等

## 五、硬件和软件
深度学习之所以能够迅猛发展，离不开强大的硬件基础，那就是GPU和CPU革命性的提升，众所周知，目前神经网络主要是在GPU上进行计算，CPU上也有但是总体比较少，因为CPU上计算速度会比GPU慢很多，大约可能会差到10-40倍速度，还有个人认为说CPU是在串行计算上占据优势，而GPU在并行计算上占有占有优势，可以去看一下CPU和GPU的硬件参数，可以发现GPU的核心数是可以达到上千个，而CPU远不如GPU只有几个，再者，我认为神经网络很大程度上是在做矩阵的运算因此GPU在这里发挥出了他巨大的优势，目前深度学习主流的GPU就是NVIDIA，我可以说目前深度学习的GPU硬件上被NVIDIA垄断了，但不得不承认它们家确实做的很好，像现在推出的专业人工智能计算版GPU（TESLA P4和V100，TITAN系列等等）价格不菲。
在软件平台上其实就是深度学习框架，深度学习很大一部分已被Python语言占据了，现在主流的框架都是以Python为主，很多深度学习框架一开始源于高校后来逐渐走向商业化，比如Theano，caffe，Torch，Keras，Tensorflow，PaddlePaddle等等。

## 六、其余的神经网络
由于这些精力有限所以这些神经网络还是不太熟悉所以暂且先罗列出来
**1、生成式对抗神经网络
2、图神经网络
3、强化学习：Q-learning**

## 七、参考文献
**1、CS231n斯坦福大学公开课**
**2、《百面深度学习》——诸葛越**
**3、《百面机器学习》——诸葛越**

