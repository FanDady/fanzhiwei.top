---
title: 深度学习：生成式模型学习笔记
date: 2021-07-06 11:30:52
img: /images/3.png
author: Yuri
top: false
summary: 本文主要记录在学习生成式模型时的一些关键知识点以及学习过程的笔记，笔记中主要是基于斯坦福大学的CS231n课程所记录，若有错误之处还望指出并谅解。
categories: 深度学习
tags: 
   - 深度学习
---
## 前言
本文主要记录在学习生成式模型时的一些关键知识点以及学习过程的笔记，若有错误之处还望指出并谅解。

## 一、生成模型
基于深度学习的生成式建模的方法主要有AE、VAE、GAN这三大种，其中VAE是基于AE的基础上进行变形的生成模型，而GAN是近年来较为流行并有效的生成式方法。
> **自编码器(AE)**：AE主要由编码器和解码器组成，整个模型其实就相当于一个压缩解压的一个过程，编码器将真实数据进行压缩到低维隐空间中的隐向量，然后解码器将压缩的隐向量进行解压得到生成数据，当然在训练过程中会将生成数据和真实数据进行比较并更新参数。其实个人感觉其目的就是使得生成数据和输入的真实数据尽量相近，尽可能抓住真实数据的核心关键，如下图所示为AE的基本框架：  
![在这里插入图片描述](https://img-blog.csdnimg.cn/2021070515081395.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70)


> **变分自编码器(VAE)**：VAE就是AE的进化版本，基础结构和AE相差不大，但是在中间隐空间的编码部分并不相同，AE的主要特点是能够模仿或者说和输入的数据尽可能接近缺少创新多样性当然本身AE自编码器就是对输入数据的一个压缩编码所以这也就是AE缺点的原因所在；再说说VAE的主要变化，其主要优势就是在编码器后产生的隐向量的概率分布能够尽量接近某个特定的分布即编码器的直接输出的是所属正态分布的均值和标准差，然后根据均值和方差采样得到隐向量z，这样就可以根据采样的随机性生成具有多样性的生成图像，如下图是VAE的基本框架： ![在这里插入图片描述](https://img-blog.csdnimg.cn/20210705151545754.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70)

 > **生成对抗网络GAN**：GAN主要由生成器和判别器构成，两者分别有自己的一个独立的网络结构，生成器生成图像再交给判别器进行判别，通过这样对抗训练的方式交替优化，生成器判别器的对抗也被称为”MinMax游戏“，即判别器最大化真实样本的输出结果，最小化假样本的输出结果，而生成器则相反。如下图所示：
 > ![在这里插入图片描述](https://img-blog.csdnimg.cn/20210705155850826.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70)

## 二、VAE和GAN之间的区别
> 首先GAN和VAE之间在输入数据和生成数据上存在不同，VAE需要两者一一对应方可求其重构损失/误差，而GAN中不需要二者一一对应；其次VAE中隐向量事实上是对输入数据特征的一种表达，在抽象的语义层面上比如改变人的发色特征没那么VAE可直接通过在隐变量空间上插值或嵌入式操作来实现。

## 三、原始GAN的优化目标
> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20210705160911581.png)

## 四、原始GAN存在的问题
> **梯度消失**：原始GAN的优化目标如上所示，在开始训练的时候生成器一般将会较为灵敏的判断出真假数据，这样造成的结果就是目标损失函数达到饱和，梯度消失，如下图是大概的推断GAN为什么梯度消失的过程：
> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20210705170628767.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70)

> **模式坍塌**：模式坍塌指的是，在生成器生成假图片的过程中，生成器的泛化能力太弱，只能产生相似或相同的图片无法创新使得生成数据具有多样性，就好比造假钞，只单一的对某一面值的钞票进行造假却无法生成其他面值的钞票。

## 五、JS散度和KL散度
> **KL散度**的定义是两个概率分布的非对称性度量，其描述的是两个概率分布拟合的程度，若两个概率分布相同时则其KL散度为0，如下为KL散度公式：  
> ![在这里插入图片描述](https://img-blog.csdnimg.cn/2021070517190799.png)

> **JS散度**的定义是基于KL散度上，但是JS散度主要是用来度量两个概率分布的相似度且JS散度是对称的其值分布在0-1之间，并且JS散度用于原始GAN中的损失函数的推导上，如下为JS散度公式：
> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20210705172400301.png)  
> 顺便附上GAN和JS散度之间的关系，其中该目标是使得D的目标函数值最大化（摘自GAN原作者的证明）
> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20210705173223192.png)


## 六、凸优化
>  **凸函数**非正式定义：函数内任取集合中两个点并连线，如果线段完全被包含在函数内则为凸函数否则为非凸函数。

> 大多数的机器学习还是深度学习的问题的一些目标函数大多会转换成凸函数求解，因为凸优化的任何局部最优解即为全局最优因此凸优化更为高效，非凸优化问题较为困难，在可行域的范围内可能存在无数个局部最小点。

## 七、基于f-散度的GAN目标函数的改进
> **f-散度**：f-散度可以理解为KL散度的一般化形式(当对应的f取值为xlogx时即为KL散度)，如下公式所示：
> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20210705185641237.png)
> 当然f-散度具有以下特点：①都是非负实数到实数的映射 ②f(1)=0 ③都是凸函数
> 由于原始GAN函数中使用的是JS散度且损失函数为交叉熵损失函数因此较难使得梯度能够稳定下降使得生成数据推向真实分布，所以出现了其他的f-散度的GAN网络使得最小化生成器中目标函数的散度值以此达到生成分布接近真实分布的目的，例如LSGAN中使用的就是均方误差的f函数，EBGAN使用的是绝对误差的f函数。

## 八、Lipschitz连续性
> **Lipschitz连续性**：Lipschitz连续的定义较为简单，简单来说就是限制f函数的导函数绝对值不超过K其中K>=0，从定义中很明显可以看出Lipschitz连续性的作用就是为了限制连续函数的局部变化最大幅度：  
> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20210705191049763.png)

> **谱归一化**：谱归一化常用来归一化判别器的权重，目的是为了让判别器满足Lipschitz连续性，在训练过程中谱归一化会对每一层的权重做奇异值分解，并对奇异值做归一化将其限制在1内，从而使得整个GAN训练网络更加稳定不容易崩塌。

## 九、IS和FID生成数据评价指标
> **IS评价指标**：IS评价指标是指Inception Score即通过Inception网络所计算生成图像分数的方法，公式如下：
> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20210705191829418.png)  
> 其中p(y|x)是指给定一张图像将其输入InceptionV3分类网络后输出的类别概率，p(y)指的是生成的图像全部输入InceptionV3网络中得到自己的概率分布并且得到所有类别上的边缘分布。

> **FID评价指标**：FID评价指的是在InceptionV3的倒数第二层上进行一个真实样本和生产样本的特征图的均值和方差来比较，公式如下  
> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20210705192356477.png)

> **FID和IS的优缺点**：IS更多在于生成数据的质量和多样性的评分，对于生成数据和真实样本之间缺少比较，而FID正弥补了IS的这一缺点，大那是FID计算特征图的均值和方差的方法过于粗糙对于图像细节无法更好的评估。

## 十、GAN的分类
> GAN可以分为有监督和无监督的方式，其中有监督的GAN要求的是生成图像和输入图像需要一一配对，反之无监督的GAN则不需要一一配对；有监督的GAN比较经典的应用就是超分辨率重建、图像补全等，而无监督的GAN应用有图像风格迁移等









